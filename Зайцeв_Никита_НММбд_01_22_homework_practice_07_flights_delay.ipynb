{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "joW7yAKsOvbM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2OdnkKgOvbO"
   },
   "source": [
    "**1.** Какая из причин отмены рейса (`CancellationCode`) была самой частой? (расшифровки кодов можно найти в описании данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Haj6g9wOvbO",
    "outputId": "c28fb154-cfb9-4970-cace-c6dd16a267ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее частой причиной отмены рейса является ' A ' 563 -вхождениями.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79265\\AppData\\Local\\Temp\\ipykernel_17608\\1245287875.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('Наиболее частой причиной отмены рейса является', \"'\", most_common_reason,\"'\", cancellation_counts[0], '-вхождениями.')\n"
     ]
    }
   ],
   "source": [
    "plane = pd.read_csv('2008.csv')\n",
    "cancellation_counts = plane['CancellationCode'].value_counts()\n",
    "most_common_reason = cancellation_counts.index[0]\n",
    "\n",
    "print('Наиболее частой причиной отмены рейса является', \"'\", most_common_reason,\"'\", cancellation_counts[0], '-вхождениями.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-1bIhqOSJFe"
   },
   "source": [
    "Загружаем датасет из CSV-файла в датафрейм Pandas.\n",
    "Посчитаем количество появлений каждого кода отмены рейса с помощью метода value_counts(). Далее находим наиболее частые причины отмены рейса.\n",
    "Мы можем сделать вывод наиболее частой причины отмены рейса и количества её вхождений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsevlRqmOvbP"
   },
   "source": [
    "**2.** Найдите среднее, минимальное и максимальное расстояние, пройденное самолетом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_1NNDLDOvbP",
    "outputId": "d58e9a33-0aa8-4054-9e4e-4d04be221e69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее расстояние, пройденное самолетом: 724.51\n",
      "Минимальное расстояние, пройденное самолетом: 31\n",
      "Максимальное расстояние, пройденное самолетом: 4962\n"
     ]
    }
   ],
   "source": [
    "plane = pd.read_csv('2008.csv')\n",
    "plane = plane.dropna(subset=['Distance'])\n",
    "mean_distance = np.mean(plane['Distance'])\n",
    "min_distance = np.min(plane['Distance'])\n",
    "max_distance = np.max(plane['Distance'])\n",
    "print(f'Среднее расстояние, пройденное самолетом: {mean_distance:.2f}')\n",
    "print(f'Минимальное расстояние, пройденное самолетом: {min_distance}')\n",
    "print(f'Максимальное расстояние, пройденное самолетом: {max_distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7scNdE5-SsMY"
   },
   "source": [
    "Прочитаем датафрейм,уберем пропуски и выведем нужную нам статистическую информацию при помощи библиотеки Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K35OtOHVOvbP"
   },
   "source": [
    "**3.** Не выглядит ли подозрительным минимальное пройденное расстояние? В какие дни и на каких рейсах оно было? Какое расстояние было пройдено этими же рейсами в другие дни?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PBwRbG1c8ys",
    "outputId": "df355580-13cc-4146-eb7e-000f4527c1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дни и рейсы с минимальным пройденным расстоянием:\n",
      "       Year  Month  DayofMonth  FlightNum\n",
      "1116   2008     12          30         65\n",
      "6958   2008     12          26         65\n",
      "17349  2008      8          18         64\n",
      "27534  2008      3          11         64\n",
      "46082  2008      8           9         65\n",
      "48112  2008      2          28         64\n",
      "\n",
      "Расстояние, пройденное этими же рейсами в другие дни:\n",
      "       Year  Month  DayofMonth  Distance\n",
      "501    2008      3          20       533\n",
      "1116   2008     12          30        31\n",
      "1389   2008      3          13       680\n",
      "1517   2008      7          10       680\n",
      "2619   2008      5          23      2381\n",
      "...     ...    ...         ...       ...\n",
      "66529  2008     12          21        82\n",
      "67172  2008      3          22       533\n",
      "68264  2008      9           7       386\n",
      "68338  2008      8           3      2454\n",
      "69305  2008      1           5      1005\n",
      "\n",
      "[78 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "min_distance_flights = plane[plane['Distance'] == min_distance]\n",
    "print('Дни и рейсы с минимальным пройденным расстоянием:')\n",
    "print(min_distance_flights[['Year', 'Month', 'DayofMonth', 'FlightNum']])\n",
    "same_flightnum_flights = plane[plane['FlightNum'].apply(lambda x: x in min_distance_flights['FlightNum'].values)]\n",
    "print('\\nРасстояние, пройденное этими же рейсами в другие дни:')\n",
    "print(same_flightnum_flights[['Year', 'Month', 'DayofMonth', 'Distance']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код:\n",
    "Выбирает все рейсы, у которых пройденное расстояние минимально, и сохраняет их в переменную min_distance_flights.\n",
    "Выводит информацию о днях и рейсах с минимальным пройденным расстоянием, отображая столбцы 'Year', 'Month', 'DayofMonth' и 'FlightNum' из DataFrame min_distance_flights.\n",
    "Создает новый DataFrame same_flightnum_flights, в котором хранятся данные о рейсах с тем же номером рейса (FlightNum), что и у рейсов с минимальным пройденным расстоянием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtkgLHhSOvbQ"
   },
   "source": [
    "**4.** Из какого аэропорта было произведено больше всего вылетов? В каком городе он находится?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKWOIzBEOvbQ",
    "outputId": "cf3bd24b-4c8e-4250-bf07-adc13294d183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аэропорт с наибольшим количеством вылетов - ATL с количеством вылетов 4134.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79265\\AppData\\Local\\Temp\\ipykernel_17608\\3229923005.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  number_of_flights = counts_by_airport[0]\n"
     ]
    }
   ],
   "source": [
    "counts_by_airport = plane['Origin'].value_counts()\n",
    "most_frequent_airport = counts_by_airport.index[0] \n",
    "number_of_flights = counts_by_airport[0]\n",
    "print(f'Аэропорт с наибольшим количеством вылетов - {most_frequent_airport} с количеством вылетов {number_of_flights}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZNR75-edcm9"
   },
   "source": [
    "\n",
    "Подсчетаем количество вылетов из каждого аэропорта с помощью метода value_counts().Находим аэропорта с наибольшим количеством вылетов путем извлечения первого индекса с наибольшим количеством вылетов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA_1A1HYOvbQ"
   },
   "source": [
    "**5.** Найдите для каждого аэропорта среднее время полета (`AirTime`) по всем вылетевшим из него рейсам. Какой аэропорт имеет наибольшее значение этого показателя?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQPG4RrAOvbR",
    "outputId": "8040c578-1a1a-4865-93db-4c1ce7f21cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аэропорт с наибольшим средним временем полета - SJU со значением 205.2.\n"
     ]
    }
   ],
   "source": [
    "average_airtime_by_airport = plane.groupby('Origin')['AirTime'].mean()\n",
    "airport_with_max_airtime = average_airtime_by_airport.idxmax()\n",
    "max_airtime_value = average_airtime_by_airport.max()\n",
    "print(f'Аэропорт с наибольшим средним временем полета - {airport_with_max_airtime} со значением {max_airtime_value}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmNLVJgGfIOU"
   },
   "source": [
    "Группируем данные по аэропортам (Origin) с помощью метода groupby() и вычисление среднего времени полета (AirTime) с помощью метода mean(). И находим аэропорта с наибольшим средним временем полета с помощью метода idxmax(), который возвращает индекс максимального значения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NDKMnQAOvbR"
   },
   "source": [
    "**6.** Найдите аэропорт, у которого наибольшая доля задержанных (`DepDelay > 0`) рейсов. Исключите при этом из рассмотрения аэропорты, из которых было отправлено меньше 1000 рейсов (используйте функцию `filter` после `groupby`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDCcuGRtOvbR",
    "outputId": "4a733fae-40f0-4945-e736-95ece68b3edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аэропорт с наибольшей долей задержанных рейсов: EWR\n",
      "Доля задержанных рейсов в этом аэропорте: 0.5111591072714183\n"
     ]
    }
   ],
   "source": [
    "grouped_airports = plane.groupby('Origin')\n",
    "filtered_airports = grouped_airports.filter(lambda x: len(x) > 1000)\n",
    "delayed_flights_percentage = filtered_airports.groupby('Origin')['DepDelay'].apply(lambda x: (x > 0).mean())\n",
    "airport_with_highest_delay_percentage = delayed_flights_percentage.idxmax()\n",
    "highest_delay_percentage = delayed_flights_percentage.max()\n",
    "\n",
    "print(\"Аэропорт с наибольшей долей задержанных рейсов:\", airport_with_highest_delay_percentage)\n",
    "print(\"Доля задержанных рейсов в этом аэропорте:\", highest_delay_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код:\n",
    "Группирует данные по исходному аэропорту отправления рейсов и сохраняет результат в переменную grouped_airports.\n",
    "Фильтрует группы аэропортов, оставляя только те, у которых количество рейсов больше 1000, и сохраняет результат в переменную filtered_airports.\n",
    "Вычисляет процент задержанных рейсов для каждого аэропорта из отфильтрованных данных, сгруппированных по исходному аэропорту, и сохраняет результат в переменной delayed_flights_percentage. Для этого используется функция apply(lambda x: (x > 0).mean()), которая считает долю положительных значений (задержанных рейсов) в столбце 'DepDelay'.\n",
    "Находит аэропорт с наивысшим процентом задержек и сохраняет его код в переменную airport_with_highest_delay_percentage.\n",
    "Находит самый высокий процент задержек среди всех аэропортов и сохраняет его значение в переменной highest_delay_percentage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dilyts_tOvbR"
   },
   "source": [
    "## Линейная регрессия\n",
    "\n",
    "В этой части мы разберемся с линейной регрессией, способами её обучения и измерением качества ее прогнозов.\n",
    "\n",
    "Будем рассматривать датасет из предыдущей части задания для предсказания времени задержки отправления рейса в минутах (DepDelay). Отметим, что под задержкой подразумевается не только опоздание рейса относительно планируемого времени вылета, но и отправление до планируемого времени.\n",
    "\n",
    "### Подготовка данных\n",
    "\n",
    "**7.** Считайте выборку из файла при помощи функции pd.read_csv и ответьте на следующие вопросы:\n",
    "   - Имеются ли в данных пропущенные значения?\n",
    "   - Сколько всего пропущенных элементов в таблице \"объект-признак\"?\n",
    "   - Сколько объектов имеют хотя бы один пропуск?\n",
    "   - Сколько признаков имеют хотя бы одно пропущенное значение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pyxaAIhOvbS",
    "outputId": "9160d6e0-aa9b-4eb2-fb28-6be9c63f039b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имеются ли в данных пропущенные значения: True\n",
      "Общее количество пропущенных элементов в таблице \"объект-признак\": 355215\n",
      "Количество объектов с хотя бы одним пропуском: 70000\n",
      "Количество признаков с хотя бы одним пропущенным значением: 16\n"
     ]
    }
   ],
   "source": [
    "plane = pd.read_csv('2008.csv')\n",
    "missing_values = plane.isnull().values.any()\n",
    "total_missing = plane.isnull().sum().sum()\n",
    "objects_with_missing = plane.isnull().any(axis=1).sum()\n",
    "features_with_missing = plane.isnull().any(axis=0).sum()\n",
    "print(f'Имеются ли в данных пропущенные значения: {missing_values}')\n",
    "print(f'Общее количество пропущенных элементов в таблице \"объект-признак\": {total_missing}')\n",
    "print(f'Количество объектов с хотя бы одним пропуском: {objects_with_missing}')\n",
    "print(f'Количество признаков с хотя бы одним пропущенным значением: {features_with_missing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0kPyyHnhEbd"
   },
   "source": [
    " Проверяем наличия пропущенных значений в данных с помощью метода isnull() и оператора any().Подсчет общего количества пропущенных элементов в таблице \"объект-признак\" с помощью метода isnull() и оператора sum().\n",
    "Подсчет объектов с хотя бы одним пропуском с помощью метода isnull(), оператора any() и оператора sum(),а также подсчет признаков с хотя бы одним пропущенным значением с помощью метода isnull(), оператора any() и оператора sum().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gglKZM3OvbS"
   },
   "source": [
    "Как вы понимаете, также не имеет смысла рассматривать при решении поставленной задачи объекты с пропущенным значением целевой переменной. В связи с этим ответьте на следующие вопросы и выполните соответствующие действия:\n",
    "- Имеются ли пропущенные значения в целевой переменной?\n",
    "- Проанализируйте объекты с пропущенными значениями целевой переменной. Чем вызвано это явление? Что их объединяет? Можно ли в связи с этим, на ваш взгляд, исключить какие-то признаки из рассмотрения? Обоснуйте свою точку зрения.\n",
    "\n",
    "Исключите из выборки объекты **с пропущенным значением целевой переменной и со значением целевой переменной, равным 0**, а также при необходимости исключите признаки в соответствии с вашим ответом на последний вопрос из списка и выделите целевую переменную в отдельный вектор, исключив её из матрицы \"объект-признак\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QVmo_E2OvbS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jISl0ZfQOvbT"
   },
   "source": [
    "**8.** Обратите внимание, что признаки DepTime, CRSDepTime, ArrTime, CRSArrTime приведены в формате hhmm, в связи с чем будет не вполне корректно рассматривать их как вещественные.\n",
    "\n",
    "Преобразуйте каждый признак FeatureName из указанных в пару новых признаков FeatureName\\_Hour, FeatureName\\_Minute, разделив каждое из значений на часы и минуты. Не забудьте при этом исключить исходный признак из выборки. В случае, если значение признака отсутствует, значения двух новых признаков, его заменяющих, также должны отсутствовать.\n",
    "\n",
    "Например, признак DepTime необходимо заменить на пару признаков DepTime_Hour, DepTime_Minute. При этом, например, значение 155 исходного признака будет преобразовано в значения 1 и 55 признаков DepTime_Hour, DepTime_Minute соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA_yDZF5OvbT",
    "outputId": "c1880e5e-1e6e-4950-ffab-648bc86951cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Month  DayofMonth  DayOfWeek UniqueCarrier  FlightNum TailNum  \\\n",
      "0  2008      6          18          3            WN        242  N699SW   \n",
      "1  2008      6           4          3            XE       2380  N15980   \n",
      "2  2008      8           3          7            WN       1769  N464WN   \n",
      "3  2008      1          23          3            OO       3802  N465SW   \n",
      "4  2008      5           4          7            WN        399  N489WN   \n",
      "\n",
      "   ActualElapsedTime  CRSElapsedTime  AirTime  ...  SecurityDelay  \\\n",
      "0               57.0            65.0     46.0  ...            NaN   \n",
      "1              124.0           138.0    108.0  ...            NaN   \n",
      "2              138.0           155.0    125.0  ...            NaN   \n",
      "3              102.0           111.0     79.0  ...            NaN   \n",
      "4              148.0           160.0    136.0  ...            NaN   \n",
      "\n",
      "   LateAircraftDelay DepTime_Hour DepTime_Minute  CRSDepTime_Hour  \\\n",
      "0                NaN         21.0           11.0               20   \n",
      "1                NaN         14.0           26.0               14   \n",
      "2                NaN         11.0           43.0               11   \n",
      "3                NaN         11.0           41.0               11   \n",
      "4                NaN          8.0           15.0                8   \n",
      "\n",
      "   CRSDepTime_Minute  ArrTime_Hour  ArrTime_Minute CRSArrTime_Hour  \\\n",
      "0                 55          23.0             8.0              23   \n",
      "1                 10          17.0            30.0              17   \n",
      "2                 45          15.0             1.0              15   \n",
      "3                 44          13.0            23.0              13   \n",
      "4                 20          12.0            43.0              13   \n",
      "\n",
      "   CRSArrTime_Minute  \n",
      "0                  0  \n",
      "1                 28  \n",
      "2                 20  \n",
      "3                 35  \n",
      "4                  0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "plane = pd.read_csv('2008.csv')\n",
    "for feature in ['DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime']:\n",
    "    plane[feature + '_Hour'] = plane[feature].apply(lambda x: x // 100 if x else np.nan)\n",
    "    plane[feature + '_Minute'] = plane[feature].apply(lambda x: x % 100 if x else np.nan)\n",
    "plane = plane.drop(['DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime'], axis=1)\n",
    "print(plane.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPM5hAHPOvbT"
   },
   "source": [
    "**9.** Некоторые из признаков, отличных от целевой переменной, могут оказывать чересчур значимое влияние на прогноз, поскольку по своему смыслу содержат большую долю информации о значении целевой переменной. Изучите описание датасета и исключите признаки, сильно коррелирующие с ответами. Ваш выбор признаков для исключения из выборки обоснуйте. Кроме того, исключите признаки TailNum и Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6hfFhjqOvbT",
    "outputId": "fd94b8c5-ff60-4cd6-fa40-6f76cd36940a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
      "0      6          18          3   2111.0        2055   2308.0        2300   \n",
      "1      6           4          3   1426.0        1410   1730.0        1728   \n",
      "2      8           3          7   1143.0        1145   1501.0        1520   \n",
      "3      1          23          3   1141.0        1144   1323.0        1335   \n",
      "4      5           4          7    815.0         820   1243.0        1300   \n",
      "\n",
      "   FlightNum  ActualElapsedTime  CRSElapsedTime  AirTime  Distance  TaxiIn  \\\n",
      "0        242               57.0            65.0     46.0       307     3.0   \n",
      "1       2380              124.0           138.0    108.0       834     4.0   \n",
      "2       1769              138.0           155.0    125.0       997     4.0   \n",
      "3       3802              102.0           111.0     79.0       532     4.0   \n",
      "4        399              148.0           160.0    136.0      1090     4.0   \n",
      "\n",
      "   TaxiOut  Cancelled  Diverted  WeatherDelay  NASDelay  SecurityDelay  \n",
      "0      8.0          0         0           NaN       NaN            NaN  \n",
      "1     12.0          0         0           NaN       NaN            NaN  \n",
      "2      9.0          0         0           NaN       NaN            NaN  \n",
      "3     19.0          0         0           NaN       NaN            NaN  \n",
      "4      8.0          0         0           NaN       NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "plane = pd.read_csv('2008.csv')\n",
    "plane = plane.drop(['TailNum', 'Year'], axis=1)\n",
    "plane = plane.select_dtypes(exclude=['object'])\n",
    "correlations = plane.corr()['ArrDelay'].abs().sort_values(ascending=False)\n",
    "highly_correlated_features = correlations[correlations > 0.5].index.tolist()\n",
    "plane = plane.drop(highly_correlated_features, axis=1)\n",
    "print(plane.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fh9Yu184eDr"
   },
   "source": [
    "\n",
    "Исключяем признаки 'TailNum' и 'Year' из датасета, так как они не будут использоваться в анализе.\n",
    "Исключение категориальные признаков .Вычисление матрицы корреляций между числовыми признаками и целевой переменной 'ArrDelay', затем сортировка по абсолютным значениям корреляции в порядке убывания.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaaTypy8OvbU"
   },
   "source": [
    "**10.** Приведем данные к виду, пригодному для обучения линейных моделей. Для этого вещественные признаки надо отмасштабировать, а категориальные — привести к числовому виду. Также надо устранить пропуски в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPvKntrHOvbU"
   },
   "source": [
    "В первую очередь поймем, зачем необходимо применять масштабирование. Следующие ячейки с кодом построят гистограммы для 3 вещественных признаков выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "pARpl4C5OvbU",
    "outputId": "2bb5dd6f-9c53-4d1b-9bd8-5cc0eb9d399c",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepTime_Hour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhist(bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X['DepTime_Hour'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3cIgWcuOvbU"
   },
   "outputs": [],
   "source": [
    "X['TaxiIn'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yb0BOb_FOvbU"
   },
   "outputs": [],
   "source": [
    "X['FlightNum'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPj9rVK_OvbV"
   },
   "source": [
    "Какую проблему вы наблюдаете на этих графиках? Как масштабирование поможет её исправить?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "28zvXyziOvbV"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ig8h_DreOvbV"
   },
   "source": [
    "Некоторые из признаков в нашем датасете являются категориальными. Типичным подходом к работе с ними является бинарное, или [one-hot-кодирование](https://en.wikipedia.org/wiki/One-hot).\n",
    "\n",
    "Реализуйте функцию transform_data, которая принимает на вход DataFrame с признаками и выполняет следующие шаги:\n",
    "1. Замена пропущенных значений на нули для вещественных признаков и на строки 'nan' для категориальных.\n",
    "2. Масштабирование вещественных признаков с помощью [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "3. One-hot-кодирование категориальных признаков с помощью [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) или функции [pd.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html).\n",
    "\n",
    "Метод должен возвращать преобразованный DataFrame, который должна состоять из масштабированных вещественных признаков и закодированных категориальных (исходные признаки должны быть исключены из выборки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "a5wkq2AxOvbV"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def transform_data(plane):\n",
    "    data_numeric = plane.select_dtypes(include=['float64', 'int64']).fillna(0)\n",
    "    data_categorical = plane.select_dtypes(include=['object']).fillna('nan')\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_numeric)\n",
    "    encoder = DictVectorizer(sparse=False)\n",
    "    data_encoded = encoder.fit_transform(data_categorical.T.to_dict().values())\n",
    "    transformed_data = pd.DataFrame(data_scaled, columns=data_numeric.columns)\n",
    "    transformed_data = pd.concat([transformed_data, pd.DataFrame(data_encoded, columns=encoder.get_feature_names_out())], axis=1)\n",
    "    return transformed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYI_tTk-58ZD"
   },
   "source": [
    "Эта функция:\n",
    "1. Заменяет пропущенные значения на нули для вещественных признаков и на 'nan' для категориальных.\n",
    "2. Масштабирует вещественные признаки с помощью StandardScaler.\n",
    "3. Кодирует категориальные признаки с помощью DictVectorizer.\n",
    "4. Возвращает преобразованный DataFrame с масштабированными вещественными и закодированными категориальными признаками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8GAAyb4OvbV"
   },
   "source": [
    "Примените функцию transform_data к данным. Сколько признаков получилось после преобразования?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6m5ztSPxOvbW",
    "outputId": "7106f331-e623-4b9f-c6a3-10dc404aee7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество признаков после преобразования: 5765\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plane = pd.read_csv(\"2008.csv\")\n",
    "transformed_data = transform_data(plane)\n",
    "print(\"Количество признаков после преобразования:\", transformed_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFfH3VVf-F8Z"
   },
   "source": [
    "Используем функцию read_csv из библиотеки pandas для загрузки данных из файла \"2008.csv\" и сохраняем их в переменную data\n",
    "Применение функции transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7-XofRAOvbW"
   },
   "source": [
    "**11.** Разбейте выборку и вектор целевой переменной на обучение и контроль в отношении 70/30 (для этого можно использовать, например, функцию [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2YSFOVoOvbW",
    "outputId": "33e669d9-395c-41f5-bff1-01bf54b7d33d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающего набора данных: (49000, 5764) (49000,)\n",
      "Размер тестового набора данных: (21000, 5764) (21000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = transformed_data.drop('ArrDelay', axis=1)\n",
    "y = transformed_data['ArrDelay']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(\"Размер обучающего набора данных:\", X_train.shape, y_train.shape)\n",
    "print(\"Размер тестового набора данных:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-mX84-OOvbW"
   },
   "source": [
    "### Scikit-learn\n",
    "\n",
    "<img src = \"https://pp.vk.me/c4534/u35727827/93547647/x_d31c4463.jpg\">\n",
    "Теперь, когда мы привели данные к пригодному виду, попробуем решить задачу при помощи метода наименьших квадратов. Напомним, что данный метод заключается в оптимизации функционала $MSE$:\n",
    "\n",
    "$$MSE(X, y) = \\frac{1}{l} \\sum_{i=1}^l (<w, x_i> - y_i)^2 \\to \\min_{w},$$\n",
    "\n",
    "где $\\{ (x_i, y_i ) \\}_{i=1}^l$ — обучающая выборка, состоящая из $l$ пар объект-ответ.\n",
    "\n",
    "Заметим, что решение данной задачи уже реализовано в модуле sklearn в виде класса [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression).\n",
    "\n",
    "**12.** Обучите линейную регрессию на 1000 объектах из обучающей выборки и выведите значения $MSE$ и $R^2$ на этой подвыборке и контрольной выборке (итого 4 различных числа). Проинтерпретируйте полученный результат — насколько качественные прогнозы строит полученная модель? Какие проблемы наблюдаются в модели?\n",
    "\n",
    "**Подсказка**: изучите значения полученных коэффициентов $w$, сохраненных в атрибуте coef_ объекта LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XY9TxSkYuJE9"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.08 GiB for an array with shape (97000, 5764) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Создание экземпляра OneHotEncoder и преобразование всех уникальных значений категориального признака\u001b[39;00m\n\u001b[0;32m     19\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder()\n\u001b[1;32m---> 20\u001b[0m X_combined_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_combined\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Разделение обратно на обучающий и тестовый наборы данных\u001b[39;00m\n\u001b[0;32m     23\u001b[0m X_train_encoded \u001b[38;5;241m=\u001b[39m X_combined_encoded[:\u001b[38;5;28mlen\u001b[39m(X_train)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1023\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1022\u001b[0m }\n\u001b[1;32m-> 1023\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:199\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 199\u001b[0m X_int \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m X_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((n_samples, n_features), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    202\u001b[0m columns_with_unknown \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.08 GiB for an array with shape (97000, 5764) and data type int32"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "X_train_subset = X_train[:1000]\n",
    "y_train_subset = y_train[:1000]\n",
    "X_test = X_train[1000:]\n",
    "y_test = y_train[1000:]\n",
    "model.fit(X_train_subset, y_train_subset)\n",
    "predictions = model.predict(X_test)\n",
    "X_combined = pd.concat([X_train, X_test])\n",
    "encoder = OneHotEncoder()\n",
    "X_combined_encoded = encoder.fit_transform(X_combined).toarray()\n",
    "X_train_encoded = X_combined_encoded[:len(X_train)]\n",
    "X_test_encoded = X_combined_encoded[len(X_train):]\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "y_train_pred = model.predict(X_train_encoded)\n",
    "y_test_pred = model.predict(X_test_encoded)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"MSE на обучающей выборке:\", mse_train)\n",
    "print(\"R^2 на обучающей выборке:\", r2_train)\n",
    "print(\"MSE на контрольной выборке:\", mse_test)\n",
    "print(\"R^2 на контрольной выборке:\", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baY2fnx4OvbW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "KvUc-uVDOvbk",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "А теперь ,Яков Михайлович, прочтите ошибку и уверен,что она вам знакома. У меня ни один компьютер(не самые древние) не смогли запустить все до конца. Не уверен,что следующие строчки кода правильные,так как не могу запустить эту ячейку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkGmmUflOvbk"
   },
   "outputs": [],
   "source": [
    "даже почистив вроде код и сам ноутбук все равно не работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w02R52eoOvbk"
   },
   "source": [
    "**13.** Обучение линейной регрессии.\n",
    "\n",
    "\n",
    "\n",
    "Обучите линейную регрессию с $L_1$ (Lasso) и $L_2$ (Ridge) регуляризаторами (используйте параметры по умолчанию). Посмотрите, какое количество коэффициентов близко к 0 (степень близости к 0 определите сами из разумных пределов). Постройте график зависимости числа ненулевых коэффициентов от коэффицента регуляризации (перебирайте значения по логарифмической сетке от $10^{-3}$ до $10^3$). Согласуются ли результаты с вашими ожиданиями?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT1H_zTbOvbl"
   },
   "source": [
    "Посчитайте для Ridge-регрессии следующие метрики: $RMSE$, $MAE$, $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "atac87SdOvbm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfW-DMYbOvbm"
   },
   "source": [
    "Подберите на обучающей выборке для Ridge-регрессии коэффициент регуляризации (перебирайте значения по логарифмической сетке от $10^{-3}$ до $10^3$) для каждой из метрик при помощи кросс-валидации c 5 фолдами на тех же 1000 объектах. Для этого воспользуйтесь GridSearchCV и KFold из sklearn. Постройте графики зависимости фукнции потерь от коэффициента регуляризации. Посчитайте те же метрики снова. Заметно ли изменилось качество?\n",
    "\n",
    "Для выполнения данного задания вам могут понадобиться реализованные в библиотеке объекты [LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html), [RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) и [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aF-Q9yLGOvbm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ehnh05UCOvbn",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**14.** Поиск объектов-выбросов\n",
    "\n",
    "\n",
    "Как известно, MSE сильно штрафует за большие ошибки на объектах-выбросах. С помощью cross_val_predict сделайте Out-of-Fold предсказания для обучающей выборки. Посчитайте ошибки и посмотрите на их распределение (plt.hist). Что вы видите?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU9C9rPvOvbn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
